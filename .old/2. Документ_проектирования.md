# Документ проектирования

## Общий подход

Terraform описывает *всю* инфраструктуру в нескольких `.tf`‑файлах. Мы держим всё в одном «root‑module», чтобы не усложнять. Масштабирование достигается переменной `instance_count` (по умолчанию — 2).

1. **init → plan → apply** — стандартный цикл Terraform.
2. Используем провайдер `yandex 0.116+` (поддержка NLB).
3. Все параметры, которые могут меняться, выносим в `variables.tf` (например, количество ВМ, образ, зона, публичный ключ, префикс имён).
4. Для установки Nginx применяем cloud‑init: скрипт bash в metadata «user‑data». Так любые ВМ «поднимаются» уже с работающим веб‑сервером.

## Архитектура (текстовая схема)

```
 ┌─────────────────┐        health‑check        ┌──────────────────┐
 │  yandex_lb_…    │──────── HTTP :80 ────────▶│  yandex_compute  │
 │  Network LB     │                           │  instance.web[0] │
 │  (Public IP)    │◀──── traffic :80 ─────────┤   Nginx 80/tcp   │
 │  Listener :80   │                           └──────────────────┘
 │  Backend group  │
 │    ↑ targets    │                           ┌──────────────────┐
 │  yandex_lb_tg   │──────── HTTP :80 ────────▶│  yandex_compute  │
 └─────────────────┘                           │  instance.web[1] │
                                               │   Nginx 80/tcp   │
                                               └──────────────────┘
```

## Файлы модуля

| Файл                       | Содержимое                                                        |
| -------------------------- | ----------------------------------------------------------------- |
| `main.tf`                  | Все ресурсы: сеть, подсеть, security‑group, ВМ, target‑group, NLB |
| `variables.tf`             | Переменные с описанием и типами                                   |
| `terraform.tfvars.example` | Пример значений (можно переименовать)                             |
| `outputs.tf`               | Важные выходы: публичный IP NLB, внутренние IP ВМ                 |
| `cloud-init.tpl`           | Bash‑скрипт установки Nginx (templatefile)                        |

## Компоненты

### 1. **Сеть (VPC)**

```hcl
resource "yandex_vpc_network" "this" {
  name = var.project_name
}
resource "yandex_vpc_subnet" "public" {
  v4_cidr_blocks = [var.cidr]
  zone           = var.zone
  network_id     = yandex_vpc_network.this.id
}
```

### 2. Security group

Открываем 22/tcp (SSH) и 80/tcp (HTTP) из‑вне.

### 3. ВМ (масштабируемые)

```hcl
resource "yandex_compute_instance" "web" {
  count       = var.instance_count
  name        = "${var.project_name}-web-${count.index}"
  platform_id = "standard-v2"
  resources {
    cores  = 2
    memory = 2
  }
  boot_disk {
    initialize_params {
      image_id = data.yandex_compute_image.ubuntu.id
    }
  }
  network_interface {
    subnet_id  = yandex_vpc_subnet.public.id
    nat        = true         # публичный IP на каждую ВМ (упростим доступ)
  }
  metadata = {
    ssh-keys  = "ubuntu:${file(var.ssh_pub_key)}"
    user-data = templatefile("cloud-init.tpl", {})
  }
}
```

### 4. Target‑group

```hcl
resource "yandex_lb_target_group" "web_tg" {
  name      = "${var.project_name}-tg"
  region_id = var.region
  dynamic "target" {
    for_each = yandex_compute_instance.web[*].network_interface[0].ip_address
    content {
      address   = target.value
      subnet_id = yandex_vpc_subnet.public.id
    }
  }
}
```

### 5. Network Load Balancer

```hcl
resource "yandex_lb_network_load_balancer" "nlb" {
  name = "${var.project_name}-nlb"
  listener {
    name = "http"
    port = 80
    target_port = 80
  }
  attached_target_group {
    target_group_id = yandex_lb_target_group.web_tg.id
    healthcheck {
      name = "http"
      http_options {
        port = 80
        path = "/"
      }
    }
  }
}
```

### 6. Outputs

```hcl
output "nlb_public_ip" {
  value = yandex_lb_network_load_balancer.nlb.listener[0].external_address_spec[0].address
}
```

## Риски и альтернативы

| Риск                                              | Как минимизируем                                                                                     |
| ------------------------------------------------- | ---------------------------------------------------------------------------------------------------- |
| Обе ВМ в одной зоне → не защищены от сбоя зоны    | Можно добавить переменную `zones` и создать по ВМ в каждой, плюс network‑interface на разные подсети |
| Cloud‑init может отработать дольше health‑check'а | Health‑check start‑delay или `depends_on = [null_resource.wait_for_nginx]`                           |

## Где что подключается

* ВМ → Target‑group автоматически через `dynamic target`.
* Target‑group → NLB через `attached_target_group`.
* Security‑group → ВМ и NLB (разрешить 80/22).

---

Теперь у нас есть детальная «карта», по которой будем писать Terraform‑код.
